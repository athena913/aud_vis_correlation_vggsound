#Self-supervised audio-visual correlation model


The code in this repo implements a self-supervised audio-visual correlation
model based on contrastive learning. The model has been trained and 
evaluated on VGGSound dataset.

#To train the model:

* modify the paths and configuration, if needed in config.py
* modify the path to the vggsound video frames and audio data in the files in the /data folder. This folder has the data loader and dataset impplementation.



